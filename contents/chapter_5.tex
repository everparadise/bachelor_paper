% !TEX root = ../main.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 第五章：实验评估
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{实验评估}
\label{ch:evaluation}

本章通过端到端测试比较包含性能预测能力的调度策略与开源社区、学术界与工业界的调度策略在LLM推理服务中的性能表现，来总结本文性能预测模块对调度决策的提升。
在Baseline选择上，本文选取了一些基于当前性能指标进行混合的策略，来展示对于简单性能指标的选择可能无法全面、准确刻画实例负载情况，与本文实现的性能预测模块得到预测性能指标的调度策略进行对比，展示基于性能预测的调度策略能够避免指标选取、参数调优的过程并且能够得到更优性能。
为了能够排除系统实现、架构设计对于端到端性能的影响，本文在Rust实现的统一全局调度框架上来实现各种策略，该框架已通过严格的性能校准验证其正确性和高效性。并且每种策略的实现也进行了校准，以保证策略实现与其在原框架上的语义相同，性能不慢于原框架实现。
* Company-X是世界最大的LLM服务提供商之一在生产环境中使用的策略，其采用简单的线性组合方式混合性能指标(TODO! 如图)。为了展示其最优效果，本文针对每个Workload通过实验将参数调优到了最佳性能。
* vLLM是LLM推理服务中使用最广泛的框架，采用了基于请求数混合的线性组合负载均衡策略。
* Dynamo是NVIDIA公布的推理框架，其调度策略同样基于线性组合，但是使用了与Company-X不同的性能指标，使用的两个性能指标是Prefill Token数和实例中的总Token数，调度器通过将请求指派到这两者混合得分最小的实例上，本文同样通过实验将参数调整到了最优配置。
而基于性能预测的调度策略实现如下
* Predictor使用本文实现的性能预测模块，预测得到请求在实例上执行完Prefill的时延，并选取时延最小的实例进行请求指派

在实验配置上，本文选取8*H20的DGX节点作为测试环境。使用Qwen2.5-7B的Dense模型和Qwen3-30B-A3B的MoE模型，来验证对于不同架构的模型，本文的性能预测能力同样有效。实验使用PD共置的方式部署实例，模型独立部署在一个GPU上，因此得到了8实例的推理服务集群进行端到端请求重放。
在工作负载选取上，使用全面、多样的开源负载来进行请求重放，具体选用的负载如下
\begin{itemize}
  \item Qwen Chatbot trace（TODO）
  \item Qwen Coder trace（TODO）
  \item Qwen Agent / ToolAgent trace（TODO）
\end{itemize}


\section{端到端性能比较}
\label{sec:e2e_results}


\subsection{稳态负载下的 TTFT 与吞吐}
\label{sec:steady}

图~\ref{fig:e2e_ttft} 展示了在稳态到达下，各策略的 TTFT（mean/p95/p99）对比结果（TODO: 补图）。
总体上，预测驱动策略在 PD 共置 mixed batch 场景下能够更准确地选择“更快获得可执行 prefill 机会”的实例，
从而降低 TTFT，尤其在高负载时对 p95/p99 的改善更显著（TODO: 用你的数据填充结论）。

图~\ref{fig:e2e_tput} 展示了系统吞吐（QPS）对比（TODO: 补图/表）。
预测驱动策略在不降低吞吐的前提下改善 TTFT，或在部分场景同时提升吞吐与 TTFT（取决于你的结果，TODO）。

\section{在线可用性：预测模块吞吐与延迟}
\label{sec:online_budget}

\subsection{预测吞吐与延迟统计}
\label{sec:pred_perf}

我们测量预测模块的处理能力，包括：
(1) 单次 \texttt{Predict} 的延迟（mean/p95/p99）；
(2) 在并发请求到达下的预测吞吐（predictions/sec 或 requests/sec）；
(3) 预测开销在调度关键路径中的占比（若可测，TODO）。

表~\ref{tab:pred_perf} 总结了预测模块性能（TODO: 填数值）。
总体结果显示，轻量在线模拟器（pattern-level + runtime-assisted）能够在高并发下保持稳定吞吐，
不会成为调度瓶颈（TODO: 结合你系统目标 QPS 说明）。

\begin{table}[t]
  \centering
  \caption{预测模块在线性能（TODO：填入不同版本/不同硬件上的 QPS 与延迟）。}
  \label{tab:pred_perf}
  \begin{tabular}{lccc}
    \toprule
    设置 & 预测吞吐（/s） & p95 延迟（$\mu$s/ms） & p99 延迟（$\mu$s/ms） \\
    \midrule
    V1 Full-fidelity & TODO & TODO & TODO \\
    V3 Lightweight w/o reuse & TODO & TODO & TODO \\
    V2 Lightweight w/ reuse & TODO & TODO & TODO \\
    \bottomrule
  \end{tabular}
\end{table}

\section{消融实验：同步机制与实现优化的必要性}
\label{sec:ablation}

本节回答 Q3：同步闭环与优化选择是否必要？
考虑到本科毕设篇幅，本节提供两类低成本且与贡献紧密相关的消融实验；你可根据已有数据保留其中一类或两类。

\subsection{同步闭环的必要性（建议保留）}
\label{sec:sync_ablation}

我们比较以下两种设置（TODO：根据实现情况调整）：
\begin{itemize}
  \item \textbf{Full-Sync：}每个 batch 完成后上报 \texttt{batch\_trace} 并修正 $S^{gt}$（默认）
  \item \textbf{No-Sync：}禁用 \texttt{SyncBatchResult}，模拟器仅靠自身推进（或使用固定背景负载），观察漂移
\end{itemize}

图~\ref{fig:sync_drift} 展示预测误差或端到端 TTFT tail 随时间的变化（TODO）。
结果通常表现为：No-Sync 会产生累积漂移，导致预测失真并损害调度收益；Full-Sync 能将漂移控制在可接受范围内。

\subsection{prediction reuse 的影响（建议保留）}
\label{sec:reuse_ablation}

我们在相同调度策略与相同 workload 下，对比 Lightweight w/ reuse（V2）与 w/o reuse（V3）：
尽管端到端吞吐可能受其他模块瓶颈限制而近似不变，
prediction reuse 仍可显著降低预测延迟分位数（例如 p99），并减少预测侧 CPU 压力（若可测，TODO）。
图~\ref{fig:reuse_latency} 给出了 V2 与 V3 的预测延迟对比（TODO），用于支持实现章节中的结论。

\section{现象分析（可选）：跨预测表的排序一致性}
\label{sec:crosstable}

本节用于解释你已观察到的现象：使用不同模型/不同预测表进行调度，mean TTFT 变化不大，但 p95/p99 差异更明显。
由于在线调度主要依赖“实例间相对优劣”的比较，而非绝对值精确，
我们用 Spearman 秩相关系数衡量两张预测表在候选实例排序上的一致程度。

\subsection{Spearman 秩相关与选择一致率}
\label{sec:spearman}

对于每个请求 $r$，我们记录候选实例集合 $\mathcal{I}$ 上两张预测表输出的实例评分序列 $\{s_A(r,i)\}$ 与 $\{s_B(r,i)\}$（本文默认 $s=\widehat{\mathrm{TTFT}}$）。
将评分转为排序后计算 Spearman 相关系数 $\rho(r)$，并统计全体请求的平均/分位数（TODO: 公式可简述）。
同时，我们计算两张预测表的 Top-1 选择一致率：
\[
\mathrm{Agree@1} = \frac{1}{|R|}\sum_{r\in R} \mathbb{1}\Big[\arg\min_i s_A(r,i) = \arg\min_i s_B(r,i)\Big]
\]
表~\ref{tab:crosstable} 给出统计结果（TODO）。

\begin{table}[t]
  \centering
  \caption{跨预测表一致性分析（TODO：填 Spearman 与 Agree@1；可再给 tail 子集统计）。}
  \label{tab:crosstable}
  \begin{tabular}{lcc}
    \toprule
    指标 & 全体请求 & tail 子集（例如 TTFT top 10\%） \\
    \midrule
    Spearman $\rho$（均值/中位数） & TODO & TODO \\
    Agree@1 & TODO & TODO \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{讨论：为何 tail 更敏感}
\label{sec:tail_sensitive}

当系统处于拥塞区间或 mixed batch 的竞争更激烈时，
不同预测表之间的绝对误差与排序误差更容易导致“边界实例”发生选择翻转，从而主要体现在 tail（p95/p99）差异上。
这一现象提示：在 PD 共置场景中，预测表的可迁移性对均值可能更鲁棒，但对尾部质量仍需谨慎评估（TODO: 结合你的数据做出结论）。
