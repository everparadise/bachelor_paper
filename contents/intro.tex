% !TEX root = ../main.tex

\chapter{绪论}
\label{ch:intro}

\section{研究背景与动机}
随着大语言模型（Large Language Models, LLMs）在智能对话、代码生成与智能体系统等应用场景中的能力被广泛验证，
各行业对大模型推理服务的调用需求迅速增长。
另一方面，用于大模型部署的高端 GPU 硬件与配套基础设施成本高昂（TODO: 给出价格/成本或引用），
使得绝大多数个人与中小企业难以独立建设稳定的大模型推理服务。
因此，模型即服务（Model-as-a-Service, MaaS）成为主流形态：服务厂商通过集中式集群部署向外部用户提供推理能力，
并以请求时延与吞吐作为核心服务质量指标（Service Level Objective/Agreement, SLO/SLA）。

在 MaaS 场景中，如何在有限 GPU 资源下提升系统吞吐、降低请求时延，并且满足用户服务质量指标(SLO)
成为 AI 系统架构与推理服务工程中的关键问题。
全局调度器（Global Scheduler）作为集群入口组件，负责将到达集群的请求路由至合适的推理实例执行，
其决策质量直接影响整体服务质量。

\section{推理集群部署与全局调度}
\subsection{集群部署形态}
在实际部署中，LLM 推理服务通常以集群形式运行，通过复制推理实例并水平扩展硬件来提升服务能力（图~TODO:cluster-arch）。
每个推理实例作为独立的模型部署单元，在 GPU 上执行推理计算，并通过批处理（batching）来管理算力资源、提升吞吐。
在推理实例之上，全局调度器将外部请求路由到不同实例；实例内部则由实例调度器（Instance Scheduler）决定如何将多个请求组成批并执行。

对于 ChatGPT、Copilot 等交互型应用，请求首 token 延迟（Time To First Token, TTFT）与每 token 生成间隔（Time Per Output Token, TPOT）
直接影响用户体验，因此服务厂商往往围绕这类指标进行优化与承诺。

\subsection{全局调度的挑战}
全局调度器需要在请求到达时快速做出路由决策,有效的路由决策能够通过选择请求指派的目标实例，平衡负载均衡和特定优化目标(例如前缀缓存)，
从而提升集群的整体服务能力。
现有的全局调度工作有两种方法来达到这个目标：一种方法是基于一些直接的实例负载指标（例如等待队列长度、总请求数量、累计 tokens、GPU 利用率等），
通过简单计算进行混合和过滤来选择目标实例。
该类方法实现简单、部署广泛（TODO: 引用/系统名称），但存在两个局限：
(1) 简单地混合指标往往难以全面刻画实例真实负载；
(2) 混合权重与阈值依赖人工经验或网格搜索，容易产生次优配置，难以充分挖掘系统潜力。

另一类方法尝试通过一些间接地请求级指标，例如预测请求在对应实例上的执行性能（如 TTFT/TPOT 或完成时间），并据此做调度决策（TODO: 引用/系统名称）。
这类策略看起来在目标上更贴近最终服务质量，但其关键前提是需要在在线调度路径中提供\emph{快速、可用且足够准确}的性能预测能力。
本文围绕这一关键问题展开：如何在在线推理服务中实现性能预测，并评估其对端到端服务质量的影响。

\subsection{Prefill/Decode 分离与共置部署形态}
\label{sec:pd_modes_intro}

在 LLM 推理服务中，请求的计算过程可分为 Prefill 与 Decode 两个阶段。
工业界与学术界的系统实现中，常见两类部署形态：

\textbf{(1) PD 分离（Prefill/Decode disaggregation）}：将 Prefill 与 Decode 分别放在不同的 GPU 实例（或不同的资源池）上执行。
该方案的优势在于：Prefill 与 Decode 的资源需求与性能特征不同，分离后可以分别进行批处理与容量规划，
让全局调度器能够用相对简单的指标来刻画每个阶段的实例负载。

\textbf{(2) PD 共置（Prefill/Decode colocation）}：Prefill 与 Decode 在同一推理实例、同一 GPU 资源池上混合执行，
实例调度器在每个调度步中同时处理部分 Prefill 与 Decode 请求，形成混合批（mixed batch）。
PD 共置下 Prefill 与 Decode 会在同一 GPU 上竞争计算与显存资源，让
因此，PD 共置场景下实例负载与TTFT / TPOT的关联更加复杂，简单的负载指标往往难以准确反映实例的服务能力空余。

本文的在线预测与调度研究主要面向 PD 共置（mixed batch）部署形态，
原因在于该场景中 Prefill 与 Decode 的相互影响使得实例负载状况较为复杂，调度决策更依赖对实例当前状态与未来批处理行为的综合判断。
相比之下，在 PD 分离场景中，分别为 Prefill 池与 Decode 池设计更简单的负载指标与路由策略。

\textbf{现实意义。}
尽管 PD 分离近年来受到广泛关注，但当前的大模型服务厂商与开源系统中仍存在 PD 共置的部署：
一方面，PD 分离需要额外的系统改造与跨机通信（包括请求状态迁移、KVCache 传递或重计算等），对工程复杂度与部署稳定性提出更高要求；
另一方面，在一定规模与成本约束下，PD 共置依然是实际可落地、可维护的部署方案（TODO: 工业经验/引用）。
因此，面向 PD 共置场景研究可同步、轻量的在线性能预测与调度策略具有现实价值。


\section{性能模拟器与在线预测}
\subsection{性能模拟器的组成}
LLM性能模拟器是评估与分析 AI 服务系统的重要工具。

按照目标场景的不同，已有的模拟器工作可以分为如下三类:
1. 训练场景
2. 离线推理场景通常在 CPU 上模拟大规模 GPU 集群执行过程，在给定系统配置与工作负载情况下离线模拟服务情况，得到吞吐、请求性能等指标。
3. 在线推理服务

% 从实现角度看，推理模拟器通常包含两个核心组件：

% \textbf{(1) 调度与状态管理组件（Scheduler）}：维护推理实例状态（请求队列、运行集合、KVCache 相关状态等），
% 并模拟实例调度器的行为，以步进方式构造批处理。
% 在离线模拟中，若实例调度器行为确定（例如常见的 vLLM 调度路径），则模拟器能够复现与真实系统一致的批构造过程。

% \textbf{(2) 执行时间预测组件（Predictor）}：对给定批处理预测其执行耗时。
% 已有工作使用不同性能模型（线性模型、学习模型、查表模型等）来拟合 batch 的执行时延（TODO: 引用）。
% 本文不以性能模型本身为主要贡献点，预测器实现参考 Vidur 等工作中的离线标定与查表方式，
% 用于验证在线模拟器设计与优化在在线调度场景中的有效性。

\subsection{离线模拟器难以直接用于在线调度}
尽管离线模拟器在系统设计与容量规划中非常有效，但直接将其搬到在线调度路径面临关键困难：

首先，\textbf{在线语义不同}。离线模拟器通常输入完整 workload，通过事件循环推进系统状态并输出整体性能指标；
而在线调度需要在每个请求到达时，对多个候选实例进行\emph{反事实性能预测}，返回用于比较的关键指标。

其次，\textbf{在线状态会漂移}。在线系统是分布式运行的：组件间通信和各自行为引入执行的不确定性，组件间可能出现队列乱序、观测滞后等现象，
使得调度器侧维护的模拟状态与真实实例状态发生偏离（divergence）。
若不处理状态漂移，基于模拟器的预测将逐渐失真，进而影响调度决策质量。

最后，\textbf{在线开销预算严格}。全量复刻实例内部调度与 KVCache 管理会带来高昂的预测开销，
难以满足在线调度路径对吞吐与延迟的要求。
因此，在线模拟器需要在“精细复刻”与“在线可用”之间做出权衡，并通过同步机制保证预测长期有效。

\section{本文工作与主要贡献}
本文面向 LLM 推理集群的全局调度场景，尝试以推理模拟器实现在线性能预测，并深入研究其对系统端到端服务能力的提升。
主要贡献总结如下：

\begin{enumerate}
  \item \textbf{提供PD共置场景下的预测能力}
  本文给出在线模拟器在PD共置下全局调度场景的语义与接口设计，
  包括请求写入、性能预测与状态同步（Sync）机制，
  并分析在线场景中导致模拟状态漂移的原因（如乱序、观测滞后与实例侧行为），为在线预测的可用性提供保证。

  \item \textbf{面向在线调度的优化与简化}
  我们首先实现一个 full-fidelity 的参考模拟器以对齐实例语义，但发现其预测吞吐难以满足在线调度需求。
  随后通过将状态抽象提升至 batch/pattern 粒度、将 KVCache 细节管理外包为实例上报，以及引入 prediction reuse 复用预测推进结果，
  显著降低预测开销，使模拟器能够在在线路径中稳定运行。

  \item \textbf{实验评估与影响因素分析}
  本文在真实硬件与多种模型/负载组合下进行端到端测试，
  对比基于模拟器预测的调度策略与典型调度策略（如 vLLM 默认策略及其他系统方案，TODO: 引用/版本），
  并分析预测准确性与预测吞吐对端到端服务质量（TTFT/TPOT、吞吐、尾延迟）的影响。
  实验硬件包括 8 卡 NVIDIA H20 与 8 卡 NVIDIA H100（TODO: 细节），模型包括 Qwen2.5-7B 与 Qwen3-30B-A3B，
  工作负载覆盖对话、代码与智能体场景（TODO: workload 说明）。
\end{enumerate}
