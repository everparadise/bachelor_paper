% !TEX root = ../main.tex

\chapter{绪论}
\label{ch:intro}

\section{背景}

随着大语言模型（Large Language Models，LLMs）的涌现能力在智能对话、代码生成以及智能体系统等应用场景中的广泛验证，各领域对大语言模型的调用需求呈几何倍数增长。然而用于大模型部署的硬件资源价格非常昂贵，绝大多数使用大模型的用户、企业难以独立部署所需的LLM推理服务。这为提供模型即服务（Model-as-a-Service, MaaS）的大模型厂商带来了飞速增长的模型调用需求。如何面向用户提供高效的推理服务成为当前AI系统架构的重要问题。

\section{LLM服务厂商部署方案}

\subsection{集群部署形态}

在模型即服务厂商的实际部署中，为了满足用户海量请求调用对硬件和算力的需求，LLM推理服务通常以集群形式运行。通过复制推理实例的方式可以高效、方便地扩展硬件来提高服务能力，每个推理实例作为独立的模型部署单元，通过批处理管理算力资源。在推理实例之上，全局调度器（global scheduler）将到达集群的调用请求路由至合适的推理实例执行。这对交互型应用如ChatGPT、copilots提高用户体验非常重要，也往往作为大模型服务厂商衡量集群服务能力和向用户承诺请求时延的重要指标。

\subsection{Prefill/Decode 共置部署形态}

在LLM推理服务中，请求的计算过程可分为Prefill与Decode两个阶段。Prefill阶段处理输入token并生成第一个输出token，计算密集但执行次数少；Decode阶段逐个生成后续token，访存密集且持续执行。在PD共置部署形态下，这两个阶段在同一推理实例、同一GPU资源池上混合执行，实例调度器在每个调度步中同时处理部分Prefill与Decode请求，调度形成混合批(Mixed Batch)。

两阶段在GPU上竞争计算资源与显存带宽，使得PD共置场景下实例负载与TTFT/TPOT的关联更加复杂。简单的负载指标往往难以准确反映实例的服务能力空余，调度决策更依赖对实例当前状态与未来批处理行为的综合判断。相比之下，PD分离场景将Prefill与Decode分别放在不同的GPU实例上执行，可以分别进行批处理与容量规划，负载刻画相对简单。

尽管PD分离近年来受到广泛关注，但当前大模型服务厂商与开源系统中仍存在PD共置的部署。PD分离需要额外的系统改造与跨机通信，包括请求状态迁移、KVCache传递或重计算等，对工程复杂度与部署稳定性提出更高要求。在一定规模与成本约束下，PD共置依然是实际可落地、可维护的部署方案。因此，面向PD共置场景研究调度策略仍然具有现实价值。

\section{性能模拟器与在线预测}

\subsection{LLM性能模拟器概述}

LLM性能模拟器是分析和评估AI服务系统的重要工具，通常通过在CPU上模拟大规模GPU集群的执行过程，在给定系统配置和请求trace的情况下离线得到性能指标。根据面对的场景和特点，可以将其分为两类。

训练性能模拟器主要面向训练场景，较有影响力的工作如SimAI，特点是使用重量级的网络模拟和较为简单的计算时长模拟。训练场景由于数据并行的存在会产生大量通信需求和网络开销；而对于计算，训练时的批处理模式较为固定，同一批中的所有序列长度保持相同，使得计算建模相对简单。

离线推理模拟器面向推理场景，目标是模拟集群吞吐、优化集群配置。推理时不存在数据并行，网络建模较为简单，但批处理和分块预填充等行为导致批处理的计算时延存在动态性，计算建模难度更高。

从实现角度看，模拟器可以拆分为两个核心组件。调度器（Scheduler）维护推理实例的状态信息，例如KVCache、请求队列等，通过模拟推理实例的请求调度行为，按照步进的方式构造Batch。由于常见实例调度器的主要行为是确定性的（例如vLLM），这是模拟器能够模拟实例调度行为的前提，保证了能够得到与真实实例调度相同的Batch构造结果。第二个部分是预测器（Predictor），对给定的Batch预测执行时间，通过对当前批处理的特征进行建模来得到对应的执行时间。已有工作通过不同的性能模型来建模批处理的执行时间，例如线性分析模型、神经网络、随机森林等。如何实现准确、高效的性能模型需要对于预测特征进行仔细的选取和设计，但这不是本文的讨论范围，本文的预测器实现参考当前使用较为广泛的推理模拟器系统Vidur，用于验证在线模拟器设计和优化方法的有效性。

\subsection{调度策略的两类方法}

在集群部署的全局调度器中，合理的调度策略能够通过负载均衡充分利用不同实例的计算资源，还能通过前缀匹配重用键值缓存避免重复计算，从而降低请求推理时延、提高系统吞吐。实现一个高效、兼顾负载均衡与前缀匹配的调度策略并不容易，已有的调度实现可以按照如下方式分类。

第一类是基于历史和当前服务指标的混合类方法。这类方法通过服务指标（例如Batch size、Total tokens）来描述实例负载和服务情况，通过加权组合或过滤进行筛选。这是最直接、简单的调度策略实现，能够通过调整混合参数、筛选阈值来控制对负载均衡、前缀匹配的选择权重，在工业界和学术界的服务系统中都有部署。但这类方法存在两个局限：首先，这些指标难以捕捉推理实例中真实、全面的负载情况，尤其在PD共置下Prefill与Decode的相互影响未被显式建模；其次，多个指标混合时的系数难以确定，往往通过网格搜索或人类专家启发式调优的方式设置。由于难以调参，次优的参数配置可能无法完全发掘推理系统的服务能力。代表系统包括vLLM（负载均衡优先）、ai-Dynamo和Company-X（线性组合）、AIBrix（过滤式）。

第二类是基于预测的未来指标类方法。这类方法尝试通过对性能指标进行预测（例如TTFT、TPOT）来指导请求调度决策，根据请求在对应实例的预测推理时延进行路由。这样的策略目标是对请求执行性能进行选择，请求执行性能同时也能包含实例负载、前缀匹配的因素，从而间接达到兼顾集群负载均衡的目标。代表系统包括llm-d（TTFT预测）和Mooncake（以KVCache为中心的调度）。但已有的基于预测性能的服务系统并未说明如何实现预测的能力。通过模拟器进行性能预测是一个直接、自然的想法，但基于模拟器实现性能预测调度策略的过程中存在很多问题未被阐明。

\subsection{从模拟器到在线预测的关键问题}

将离线模拟器应用于在线调度场景面临三个关键问题。

语义不同。离线模拟器通常输入完整workload，通过事件循环推进系统状态并输出整体性能指标；而在线调度需要在每个请求到达时，对多个候选实例进行反事实性能预测，返回用于比较的关键指标。两者的输入输出语义存在本质差异。

状态同步。在线系统是分布式运行的，组件间通信和各自行为引入执行的不确定性。全局调度器对推理实例的信息观测存在时延，推理实例内部的Tokenize过程对先后到达的请求不保序，这些因素可能导致模拟器维护的状态与真实实例状态发生偏移。若不处理状态同步问题，基于模拟器的预测将逐渐失真，进而影响调度决策质量。

开销预算。全量复刻实例内部调度与KVCache管理会带来高昂的预测开销，难以满足在线调度路径对吞吐与延迟的要求。因此在线模拟器需要在精细复刻与在线可用之间做出权衡，并通过同步机制保证预测长期有效。

\section{本文工作与主要贡献}

本文面向LLM推理集群的全局调度场景，尝试通过性能模拟器实现在线请求性能预估，解决实现过程中的状态同步与开销问题，并系统分析预测能力对调度决策效果的影响。具体来说，在PD共置部署形态下设计在线模拟器的语义、接口与架构，通过状态简化和Prediction Reuse大幅降低预测时延，使基于预测的调度策略能够稳定运行在在线路径中。在此基础上，本文通过端到端实验对比基于模拟器的策略与多项已有调度策略，分析预测准确性与吞吐对服务质量的影响。

主要贡献总结如下：

\begin{enumerate}
  \item \textbf{PD共置场景下的预测能力设计。} 给出在线模拟器在全局调度场景中的语义与接口定义，提出面向PD共置的模拟器架构设计，包括请求写入、性能预测与状态同步机制。分析在线场景中导致模拟状态偏移的原因（观测时延与Tokenize不保序），为在线预测的可用性提供保证。

  \item \textbf{面向在线调度的优化与简化。} 首先实现full-fidelity参考模拟器以对齐实例语义，但发现其预测吞吐难以满足在线调度需求。随后通过状态简化（将模拟粒度从token-level提升至batch-pattern level）和Prediction Reuse（复用预测推进结果），显著降低预测开销。优化后的预测平均时延控制在50微秒以下，完全满足在线路径吞吐需求，不成为系统瓶颈。

  \item \textbf{实验评估与影响因素分析。} 在8卡NVIDIA H20与8卡NVIDIA H100集群上进行端到端测试，模型覆盖Qwen2.5-7B（Dense）和Qwen3-30B-A3B（MoE），工作负载包括Qwen Chatbot、Qwen Coder、Qwen Agent和Kimi ToolAgent。对比基于模拟器的策略与vLLM-v1、NVIDIA Dynamo、Company-X、llm-d等调度策略，验证性能提升效果。分析预测准确性与吞吐对端到端服务质量（TTFT/TPOT、吞吐、尾延迟）的影响，阐明模拟器为何能在在线调度中起到帮助。
\end{enumerate}