% !TEX root = ../main.tex

\chapter{绪论}
\label{ch:intro}

大语言模型（Large Language Models, LLMs）在智能对话、代码生成和智能体系统等场景中展现出强大的涌现能力，
推动各行业对模型推理调用的需求快速增长。
由于大模型部署所需的GPU硬件资源价格昂贵，绝大多数用户和企业无法独立部署推理服务，
转而依赖模型即服务（Model-as-a-Service, MaaS）厂商提供的推理API。
如何面向海量并发请求提供高效、低延迟的推理服务，成为当前AI系统架构的核心挑战。

在MaaS厂商的实际部署中，LLM推理服务通常以集群形式运行：
多个推理实例各自独立执行模型推理，全局调度器（global scheduler）作为集群入口，
将到达的请求路由至合适的实例。
全局调度的质量直接影响用户感知的两项关键指标——
时间到首token（TTFT）和每token生成时间（TPOT），
这对交互型应用如对话助手和代码补全工具的体验至关重要。
在Prefill与Decode共置（PD共置）的部署形态下，
两个计算特性截然不同的阶段在同一GPU上混合执行、竞争资源，
使得实例负载与请求延迟之间的关联难以用简单指标刻画，
为全局调度带来了更大的挑战。

现有的调度策略可分为两类。
第一类基于当前服务指标的混合方法，通过对batch size、总token数、缓存命中率等指标
进行加权组合或阈值过滤来选择目标实例。
代表系统包括vLLM（负载均衡优先）、NVIDIA Dynamo和Company-X（线性组合）、AIBrix（过滤式）。
这类方法实现简单、部署广泛，但存在两个局限：
其一，简单指标难以准确反映PD共置下实例的真实服务能力，
batch size相同但Prefill/Decode配比不同的实例可能有截然不同的执行时间；
其二，多指标混合的权重需要针对特定workload调优，
我们的实验表明权重偏离最优值0.1即可导致平均TTFT增加15\%--25\%，
且最优权重随workload类型变化，静态调参难以适应。

第二类方法尝试预测请求在各候选实例上的执行性能（如TTFT），
以预测值作为调度依据，代表系统包括llm-d和Mooncake。
预测类方法的优势在于：预估的TTFT天然融合了负载均衡与缓存感知两个目标，
调度器只需比较单一数值即可完成决策，无需人工设定权重或阈值。
然而，已有工作均未阐明如何在在线场景中实现可用的性能预测能力。
通过性能模拟器进行预测是一个自然的思路——
模拟器通过复刻实例调度器的行为构造批处理，再由预测器对批处理特征建模得到执行时间——
但将离线模拟器应用于在线调度面临三个关键问题。

第一，\textbf{语义差异}。离线模拟器输入完整workload、输出整体性能报告；
在线调度需要的是低延迟的查询接口，为每个到达的请求对多个候选实例进行反事实性能预测，
在微秒级返回用于比较的指标。

第二，\textbf{状态同步}。在线系统中调度器与实例是分布式部署的，
调度器对实例状态的观测存在时延，实例内部的分词过程对请求不保序，
这些因素会导致模拟器维护的状态镜像与实例真实状态发生偏移。
若不处理状态同步问题，预测将逐渐失真。

第三，\textbf{开销预算}。全量复刻实例内部调度与KVCache管理会带来高昂的计算开销，
而性能预测处于全局调度的关键路径上，必须在高请求到达率下不成为系统瓶颈。

本文面向PD共置部署形态下的LLM推理集群全局调度场景，
设计并实现了在线性能预测模块，将性能模拟器从离线工具转变为在线调度组件。
在架构设计上，我们定义了面向在线调度的模拟器语义与接口（Predict、Add\_request、Sync\_State），
通过推理实例主动上报增量状态实现与模拟器的状态同步。
在性能优化上，我们提出预测复用（Prediction Reuse）机制，
通过维护演进状态复用前序请求的模拟结果，避免重复计算，
并结合状态简化将预测平均时延控制在50微秒以下，
使预测模块在64实例规模的集群上仍不成为系统瓶颈。
在实验验证上，我们在统一的Rust全局调度框架上实现了vLLM、NVIDIA Dynamo、Company-X等多种基线策略，
在8卡NVIDIA H20集群上使用Qwen2.5-7B（Dense）和Qwen3-30B-A3B（MoE）模型，
覆盖ChatBot、Coder、Agent和ToolAgent四类工作负载进行端到端请求重放测试。
实验结果表明，基于性能预测的调度策略在所有场景下均显著降低TTFT，
相比vLLM带来至多(TODO!)的TTFT Mean提升，仅牺牲(TODO!)的TPOT性能。
此外，我们发现预测表在不同模型间具有一定的迁移能力，
使用Qwen2.5-7B的预测表指导Qwen3-30B-A3B的调度仍能保持(TODO!)的决策一致率。

\section{主要贡献}

本文的主要贡献总结如下：

\begin{enumerate}
  \item \textbf{面向PD共置的在线性能预测架构设计。}
    定义在线模拟器在全局调度场景中的语义与接口，
    设计基于增量状态上报的同步机制，分析在线场景中导致模拟状态偏移的两类原因
    （观测时延与分词不保序），通过演进状态校准保证预测长期有效。

  \item \textbf{面向在线调度路径的预测优化。}
    提出预测复用机制，通过维护真实状态、演进状态与模拟状态的三层结构，
    复用前序请求的模拟推进结果，显著降低预测开销。
    优化后的预测平均时延控制在50微秒以下，满足大规模集群的在线调度需求。

  \item \textbf{系统性的端到端实验评估。}
    在统一调度框架上实现多种主流调度策略，
    在真实硬件上覆盖Dense与MoE两种模型架构、四类生产级工作负载进行端到端对比。
    验证基于预测的调度策略在TTFT上的显著优势，
    分析预测准确性、预测吞吐与跨模型迁移能力对调度效果的影响，
    为推理集群调度设计提供实证依据。
\end{enumerate}

\section{论文组织}

本文其余部分组织如下：
第二章介绍LLM推理的背景知识，包括Prefill/Decode阶段、KVCache机制、
PD共置下的混合批处理，以及现有调度策略的详细分析与动机实验。
第三章阐述在线性能预测模块的系统设计，包括架构与接口设计、状态同步机制和状态演进管理。
第四章介绍系统实现细节，包括核心数据结构和性能预测器的实现。
第五章通过端到端实验评估基于性能预测的调度策略，分析预测准确性、跨表迁移和在线可用性。
第六章总结全文并展望未来工作。