% !TEX root = ../main.tex

\chapter{背景与动机}
\label{ch:background}

\section{LLM推理基础}

\subsection{Prefill与Decode阶段}

大语言模型以自回归方式生成文本。给定输入提示（prompt），模型逐token产生输出，直到生成终止符或达到最大长度。这一过程可明确划分为两个阶段。

Prefill阶段处理输入提示中的所有token，并行计算得到第一个输出token。由于输入token可并行处理，Prefill阶段属于计算密集型操作，其执行时间主要取决于输入长度和模型参数量。完成Prefill后，输入提示的键值缓存（Key-Value Cache，简称KVCache）被保存在GPU内存中，供后续阶段复用。

Decode阶段逐token生成剩余输出。每生成一个新token，模型需要将该token作为输入执行一次前向传播，并更新KVCache。Decode阶段属于访存密集型操作，其执行时间主要受显存带宽和批次大小影响，与输出长度呈线性关系。

两个阶段的性能指标直接决定用户体验。时间到首token（Time To First Token，TTFT）衡量从请求到达至返回第一个输出token的耗时，反映系统的响应速度。时间到每token（Time Per Output Token，TPOT）衡量生成后续token的平均间隔，反映系统的流式输出流畅度。交互型应用如对话系统和代码助手对这两项指标均有严格要求。

\subsection{KV Cache机制}

KVCache是加速LLM推理的关键技术。Prefill阶段计算得到的键矩阵和值矩阵被缓存下来，Decode阶段只需计算当前token的键值，并与历史缓存拼接后参与注意力计算。这避免了每步重新计算历史token的键值，大幅降低计算量。

KVCache还带来一个重要特性：缓存命中可加速Prefill阶段。若新请求的输入前缀与某已完成请求完全匹配，且该请求的KVCache仍保留在内存中，则实例可直接复用缓存的前缀计算结果，仅需对剩余输入token执行Prefill。这称为前缀缓存命中。缓存命中的请求Prefill耗时显著降低，TTFT随之改善。

缓存命中率受多种因素影响：请求前缀的重复模式、缓存容量、缓存淘汰策略、请求路由决策等。其中路由决策由全局调度器控制，将共享相同前缀的请求导向同一实例可提高命中率，但也可能导致该实例负载过重。

\subsection{PD共置下的混合批处理}

在PD共置部署形态中，Prefill与Decode请求在同一实例的同一GPU资源池上混合执行。实例调度器在每个迭代步中从队列中选择一批请求执行，这批请求可能同时包含Prefill阶段的新请求和Decode阶段的进行中请求，形成混合批（mixed batch）。

混合批处理中两阶段存在复杂的相互影响。Prefill请求计算密集，需要访问模型参数并计算完整前向传播；Decode请求访存密集，主要从KVCache读取历史状态并计算当前token。两者在GPU上竞争计算资源（SM）和显存带宽。当混合批中Prefill请求占比较高时，Decode请求的TPOT可能显著增加；反之，大量Decode请求也会延长Prefill请求的等待时间。

现代推理引擎采用分块预填充（chunked prefill）来缓解这种相互影响。长Prefill请求被切分为多个小块，与Decode请求交错执行，避免单个大Prefill阻塞Decode过久。但分块预填充增加了调度复杂性：每个Prefill块何时执行、与哪些Decode请求组成批次，都会影响端到端延迟。

这种复杂性使得PD共置场景下的性能预测远比PD分离场景困难。PD分离将两阶段隔离开，Prefill池和Decode池的负载可分别用独立指标刻画。PD共置下，实例负载与TTFT/TPOT的关联难以用简单指标（如队列长度、batch size）准确描述，因为同一batch size下，Prefill与Decode的不同配比会产生截然不同的执行时间。这为全局调度带来了挑战：调度器需要更精细的手段来预估请求在不同实例上的实际执行性能。

尽管PD分离近年来受到广泛关注，但PD共置在当前大模型服务厂商与开源系统中仍有广泛部署。PD分离需要额外的系统改造与跨机通信支持，包括请求在Prefill实例与Decode实例之间的状态迁移、KVCache的跨节点传递或重计算等，对工程复杂度与部署稳定性提出更高要求。此外，PD分离需要独立规划两个资源池的容量，在请求模式波动较大时可能出现一侧资源不足而另一侧空闲的情况。在一定的集群规模与成本约束下，PD共置依然是实际可落地、运维成本可控的部署方案。因此，面向PD共置场景研究调度策略仍然具有重要的现实价值。

\subsection{集群级全局调度}
在模型即服务厂商的实际部署中，为了满足海量请求对硬件和算力的需求，LLM推理服务通常以集群形式运行。通过复制推理实例的方式可以高效地横向扩展服务能力，每个推理实例作为独立的模型部署单元，承载完整的模型权重并通过批处理管理本地算力资源。这种基于实例复制的扩展模式具有部署简单、故障隔离等优势，已成为业界的主流架构选择。

LLM服务集群由全局调度器和多个推理实例组成。调度器作为集群入口，接收所有外部请求，并为每个请求选择目标实例。每个实例独立维护自己的计算资源和状态信息，执行本地调度与批处理。

在请求到达时，调度器依赖实例上报的状态信息进行决策。常见上报指标包括：队列长度、运行请求数、总Token数、KVCache变化等。这些指标从实例传递至调度器以辅助进行调度决策。

端到端服务质量通常以TTFT(Time-to-First-Token)和TPOT(Time-per-Output-Token)作为主要度量指标，分别刻画了用户收到第一个回复和后续每一次回复的及时性。
这些指标通常作为云厂商对客户的SLO承诺，因此TTFT和TPOT通常作为系统服务能力的观测指标和评价依据。
调度策略的优劣将直接关系到云厂商的运营成本和客户满意度。

\section{LLM性能模拟器}

LLM性能模拟器是分析和评估AI服务系统的重要工具。其核心思想是在CPU上模拟GPU集群的执行过程：给定系统配置（模型参数、硬件规格、部署方式）和请求工作负载（请求到达时间、输入输出长度），模拟器通过步进执行的方式推进系统状态，最终输出吞吐、时延等性能指标。
相比于在真实硬件上进行实验，模拟器的成本极低，可以在几分钟内完成数小时真实工作负载的模拟，这使得大规模的参数探索和配置优化成为可能。

根据面向场景和部署特性的不同，LLM性能模拟器可分为训练性能模拟器和推理性能模拟器两类。两者由于并行配置和执行逻辑的不同，对于性能的建模方式和侧重点也有着很大的差异：
\textbf{训练性能模拟器}面向分布式训练场景，代表工作为SimAI。
分布式训练通常采用数据并行这样的大规模并行策略，这会产生大量的跨节点通信需求。
因此在大规模训练集群中，网络开销往往是性能瓶颈，训练模拟器因而侧重于精确建模网络拓扑和通信模式。
SimAI采用重量级的网络模拟器来刻画集合通信的时延，能够建模不同网络拓扑下的通信行为，并考虑网络拥塞、带宽竞争等因素对通信时间的影响。
相比之下，训练时的计算建模较为简单，因为同一个训练批次中所有序列的长度保持一致（通过padding或packing对齐），
批处理行为是静态的，每一步的计算量可以通过模型参数量和批次配置直接计算得到，不涉及动态批处理带来的计算时间波动。

\textbf{离线推理模拟器}面向推理服务场景，代表工作为Vidur，
目标是模拟推理集群在给定工作负载下的服务表现，用于指导集群配置优化和容量规划。
与训练场景不同，推理服务通常将完整的模型部署在单个或少量GPU上，
不涉及大规模的数据并行通信，因而网络开销占比较小，采用相对简单的网络建模。
但推理模拟器面临的计算建模挑战远大于训练模拟器，原因在于推理时的批处理行为是高度动态的。
在持续批处理（continuous batching）机制下，每个调度步中批次的组成都可能不同：
新请求的加入、旧请求的完成、分块预填充的切分都会改变批次的Prefill/Decode配比和总token数，而这些变化直接影响每一步的计算时间。

从实现架构来看，推理模拟器可以拆分为两个核心组件：调度器（Scheduler）和预测器（Predictor）。
调度器负责维护推理实例的完整状态信息，包括等待队列、运行集合和KVCache分配情况等，通过复刻推理实例内部的调度逻辑，按步进方式构造每一步的批处理结果。
这里存在一个关键前提：主流推理引擎（如vLLM）的实例调度器行为在给定状态下是确定性的，即相同的请求队列和KVCache状态会产生完全相同的批次构造结果。
这一确定性保证了模拟器能够准确复刻实例的调度行为，是模拟调度过程可行的理论基础。
预测器则对调度器构造出的每个批次进行执行时间预测，
通过建模批处理的关键特征（如总token数、Prefill/Decode请求数及其KVCache大小等）
来估计该批次在GPU上的实际执行时间。
已有工作探索了多种性能建模方法，包括基于Roofline的线性分析模型、神经网络回归模型、随机森林等机器学习方法，不同的建模方法在预测精度、特征需求和推理开销上各有权衡。


上述两类模拟器虽然面向不同场景，但都以离线方式运行：
输入一段完整的请求工作负载或训练配置，模拟器在CPU上完整执行一遍模拟，输出整体的性能指标报告。
这种离线运行模式足以满足系统设计阶段的容量规划和配置优化需求，但无法直接用于在线调度场景，
这是因为在线场景
如何将离线模拟器的预测能力迁移到在线场景并解决目标、语义不同带来的差异，是本文要解决的核心问题。

\section{现有调度策略分析}
前文提到在LLM推理服务中，调度策略的优劣将直接关系到云厂商的运营成本和客户满意度，因此开源社区、学术界和工业界都提出了多种调度策略来优化推理服务的性能。根据选用的指标不同，可将调度策略分为两类：


第一类方法是选定当前实例负载状态的指标，通过对这些指标的组合或过滤来综合评估实例负载情况并比较，选定最优的实例。代表工作如下:

\textbf{vLLM-v1}采用负载均衡优先的设计。调度器计算每个实例的得分：score = 4 × Q-BS + 1 × R-BS，其中Q-BS为队列中等待的请求数，R-BS为正在运行的请求数。得分最小的实例被选中。该策略完全忽略KVCache命中的影响，在缓存命中率高的场景中，这种策略会错失大量加速机会。

\textbf{ai-Dynamo}采用线性组合策略。调度器线性组合Prefill的Token数和实例总Token数来综合评估实例负载情况，通过调整权重控制两个目标的侧重。

\textbf{Company-X}同样通过线性组合混合两个实例负载指标，不过选定的指标不同，使用新请求加入实例后实例中的Prefill Token数(考虑Prefix Cache命中)和实例中总请求数进行混合

这种混合指标的方式存在两个问题：
1. 刻画实例当前负载情况需要大量的性能指标，难以通过一到两个性能指标的混合来全面、准确描述实例负载情况，例如在ai-Dynamo的指标选取下无法区分不同Decode请求数的实例，而Company-X又无法区分Decode的Token数不同。
2. 多指标混合的权重会影响性能，随着选取参数的增加，参数的选取和调优难度将以几何程度的增加，对于人工选取参数和调优的策略来说，性能的提升将越来越难以通过参数的调整来实现。

\textbf{Company-X调参敏感性分析}为说明线性组合方法在混合指标时存在调整组合参数时存在局限性，参数的变化会带来性能的明显改变，我们在Chatbot 工作负载上改变Company-X的权重参数进行网格搜索的端到端实验。
实验的硬件环境为8 × H20的服务器，使用模型为Qwen3-30B-A3B。权重参数λ从0.0变化至1.0（λ为Company-X策略中两种指标的贡献权重）。实验发现：λ值的改变会较大程度的影响请求服务质量。同一场景下，λ偏离最优值0.1导致平均TTFT增加15\%-25\%。这表明基于当前实例负载组合的策略在实际部署中对于混合参数的选择是敏感的，参数需要进行精细的调优才能达到对应指标选择下的最优性能。
对比ai-Dynamo的最优性能，同样是对两种指标的加权组合，两种策略的性能差距较为明显，这意味着基于当前实例负载组合的策略中对于选取的指标不同同样会大幅度影响性能表现。
上面的两种策略仅仅使用两种指标混合，就已经因为指标选取、参数调优带来了明显的性能差距，如果在调度指标中使用更多的指标进行混合，那么参数的选取和调优带来的性能变化将更加显著。

第二类方法通过预测请求指派到实例后实例的执行情况或服务指标，以预测值作为调度依据根据预测负载状态选取实例，而非混合实例当前的负载指标。
预测负载状态往往通过模型或模拟器预估请求在各实例上的执行情况。代表工作如下:

\textbf{llm-d}调度决策中将预测的TTFT和TPOT性能作为调度依据，并通过组合的方式平衡两者的权重。

\textbf{Mooncake}面向的部署形态为PD分离，因此调度器只需要考虑Prefill请求的执行情况。通过预测Prefill请求的执行时长和KVCache传输时长来选择TTFT最短的实例。

预测类方法的优势在于：相比于描述实例负载情况，描述请求执行性能更加简单，且与服务目标有直接的对应关系，调度器只需对服务目标进行选择即可完成决策，而不是对选取实例负载指标进行复杂的组合和调优。

但预测类方法面临实现层面的挑战，例如如何在在线场景中快速获得准确预估？预测不准确会带来多大影响？以及基于预测请求性能的策略能够带来多大的端到端收益？这些问题在已有工作中并未被深入讨论和全面回答。

两类方法的根本差异在于对调度目标的表达方式。
第一类方法描述实例的当前负载状态，通过选取和组合负载指标间接推断哪个实例更适合接收新请求，决策质量于所选指标能否准确刻画实例的真实服务能力有关。
而第二类方法则是直接描述请求的执行性能，通过预测请求在各实例上的TTFT等服务指标来选择最优实例，决策质量受预测的准确性影响，但其优势在于目标与服务质量直接对应，无需人为构造设计指标到服务质量之间的映射关系。

从信息利用角度看，第一类方法在选用指标时丢弃了大量信息，为了降低网格搜索参数最优配置的复杂度，往往无法全面的选择指标组合，而准确描述实例负载又需要大量的性能指标，这种矛盾导致了服务能力的提升严格依赖于性能指标的选取和参数调优，并且难以实现最优的性能表现。
而预测类方法通过模拟和性能建模，极大程度上利用了实例当前的各种状态信息对请求的执行情况进行预测，通过预测的形式将当前的状态信息和负载指标转化为请求执行性能并作为最终的调度依据，避免了人为的选取指标和调整参数的复杂性从而能够利用到更为全面的信息。

\section{现有方法未解决的问题}

现有工作虽已尝试将性能预测引入调度决策，但三个关键问题仍未得到清晰解答。

第一，在线场景中如何实现可用的性能预测？预测依赖实例状态，但与离线模拟器不同，在线场景中调度器维护的模拟状态如何与真实实例状态保持一致以保证预测准确？预测器如何设计才能在在线路径中稳定运行？这些问题在llm-d等工作中未被阐明。

第二，预测能力能带来多大的调度性能提升？虽然基于预测的调度策略能够组合更多样的负载指标和状态信息，但已有工作并未进行详尽全面的端到端对比实验，验证这种通过预测形式组合参数的方式相比人为选取参数和调优能够带来多大的性能提升。在真实硬件、真实工作负载上，预测类方法相比vLLM、ai-Dynamo等能否带来确定、明显的TTFT/TPOT改善？
由此确定投入成本实现预测类方法是否能够带来相当的回报。

第三，预测质量如何影响调度效果？预测不可能完全准确，但预测误差在多大范围内可接受？预测吞吐是否会成为系统瓶颈？

本文面向PD共置部署形态，尝试填补上述空白。我们设计并实现在线模拟器，解决状态同步与开销控制问题；通过端到端实验量化预测类方法的性能提升；系统分析预测准确性和吞吐对调度效果的影响，为推理集群调度设计提供实证依据。