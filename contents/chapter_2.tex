% !TEX root = ../main.tex

\chapter{背景与动机}
\label{ch:background}

\section{LLM推理基础}

\subsection{Prefill与Decode阶段}

大语言模型以自回归方式生成文本。给定输入提示（prompt），模型逐token产生输出，直到生成终止符或达到最大长度。这一过程可明确划分为两个阶段。

Prefill阶段处理输入提示中的所有token，并行计算得到第一个输出token。由于输入token可并行处理，Prefill阶段属于计算密集型操作，其执行时间主要取决于输入长度和模型参数量。完成Prefill后，输入提示的键值缓存（Key-Value Cache，简称KVCache）被保存在GPU内存中，供后续阶段复用。

Decode阶段逐token生成剩余输出。每生成一个新token，模型需要将该token作为输入执行一次前向传播，并更新KVCache。Decode阶段属于访存密集型操作，其执行时间主要受显存带宽和批次大小影响，与输出长度呈线性关系。

两个阶段的性能指标直接决定用户体验。时间到首token（Time To First Token，TTFT）衡量从请求到达至返回第一个输出token的耗时，反映系统的响应速度。时间到每token（Time Per Output Token，TPOT）衡量生成后续token的平均间隔，反映系统的流式输出流畅度。交互型应用如对话系统和代码助手对这两项指标均有严格要求。

\subsection{KV Cache机制}

KVCache是加速LLM推理的关键技术。Prefill阶段计算得到的键矩阵和值矩阵被缓存下来，Decode阶段只需计算当前token的键值，并与历史缓存拼接后参与注意力计算。这避免了每步重新计算历史token的键值，大幅降低计算量。

KVCache还带来一个重要特性：缓存命中可加速Prefill阶段。若新请求的输入前缀与某已完成请求完全匹配，且该请求的KVCache仍保留在内存中，则实例可直接复用缓存的前缀计算结果，仅需对剩余输入token执行Prefill。这称为前缀缓存命中。缓存命中的请求Prefill耗时显著降低，TTFT随之改善。

缓存命中率受多种因素影响：请求前缀的重复模式、缓存容量、缓存淘汰策略、请求路由决策等。其中路由决策由全局调度器控制，将共享相同前缀的请求导向同一实例可提高命中率，但也可能导致该实例负载过重。

\subsection{PD共置下的混合批处理}

在PD共置部署形态中，Prefill与Decode请求在同一实例的同一GPU资源池上混合执行。实例调度器在每个迭代步中从队列中选择一批请求执行，这批请求可能同时包含Prefill阶段的新请求和Decode阶段的进行中请求，形成混合批（mixed batch）。

混合批处理中两阶段存在复杂的相互影响。Prefill请求计算密集，需要访问模型参数并计算完整前向传播；Decode请求访存密集，主要从KVCache读取历史状态并计算当前token。两者在GPU上竞争计算资源（SM）和显存带宽。当混合批中Prefill请求占比较高时，Decode请求的TPOT可能显著增加；反之，大量Decode请求也会延长Prefill请求的等待时间。

现代推理引擎采用分块预填充（chunked prefill）来缓解这种相互影响。长Prefill请求被切分为多个小块，与Decode请求交错执行，避免单个大Prefill阻塞Decode过久。但分块预填充增加了调度复杂性：每个Prefill块何时执行、与哪些Decode请求组成批次，都会影响端到端延迟。

这种复杂性使得PD共置场景下的性能预测远比PD分离场景困难。PD分离将两阶段隔离开，Prefill池和Decode池的负载可分别用独立指标刻画。PD共置下，实例负载与TTFT/TPOT的关联难以用简单指标（如队列长度、batch size）准确描述，因为同一batch size下，Prefill与Decode的不同配比会产生截然不同的执行时间。这为全局调度带来了挑战：调度器需要更精细的手段来预估请求在不同实例上的实际执行性能。

\section{集群级全局调度}

\subsection{系统架构与信息流}

LLM服务集群由全局调度器和多个推理实例组成。调度器作为集群入口，接收所有外部请求，并为每个请求选择目标实例。每个实例独立维护请求队列、运行集合和KVCache，执行本地调度与批处理。

调度器依赖实例上报的状态信息进行决策。常见上报指标包括：队列长度（Q-Len）、运行请求数（R-BS）、总token数、GPU利用率等。这些指标通过心跳或响应头携带的方式从实例传递至调度器。由于网络传输和实例处理存在时延，调度器观测到的状态总是滞后于实例真实状态。此外，实例内部的Tokenize过程对先后到达的请求不保证处理顺序，可能导致调度器维护的请求顺序视图与实例实际执行顺序不一致。这种观测时延和顺序不确定性构成状态同步问题的根源。

\subsection{调度决策的两大目标}

全局调度器需要在请求到达时快速做出路由决策。有效调度追求两个有时相互冲突的目标。

负载均衡旨在避免部分实例过载而其他实例空闲。过载实例的请求排队时间增加，TTFT和TPOT随之恶化；空闲实例则浪费宝贵GPU资源。传统负载均衡策略（如Join the Shortest Queue）在LLM服务中仍有应用，但需适配LLM特有的计算特征。

缓存感知旨在利用KVCache前缀命中加速Prefill。将共享相同前缀的请求路由至同一实例可提高缓存命中率，降低TTFT。但过度追求缓存命中会导致请求堆积在少数实例上，破坏负载均衡。如何在两者间取得权衡是调度策略设计的核心难点。

\subsection{调度性能的度量}

端到端服务质量最终体现为TTFT和TPOT的分布特性。除平均值外，尾延迟（如P95、P99）对用户体验同样关键。少数请求的异常延迟可能破坏用户对整体服务的印象。

调度策略不直接产生计算延迟，而是通过影响实例负载分布和缓存命中率来间接影响端到端指标。因此评估调度策略需要端到端实验：在真实硬件上重放请求trace，测量不同策略下的TTFT/TPOT分布。单纯的模拟或理论分析不足以揭示调度策略在实际系统中的表现。

\section{现有调度策略分析}

\subsection{对当前状态Metrics的混合}

第一类方法基于实例当前的服务指标进行加权组合或过滤，直接生成调度决策。这类方法实现简单，部署广泛，但存在一些局限。

\textbf{vLLM默认策略}采用负载均衡优先的设计。调度器计算每个实例的得分：score = 4 × Q-BS + 1 × R-BS，其中Q-BS为队列中等待的请求数，R-BS为正在运行的请求数。得分最小的实例被选中。该策略完全忽略KVCache命中的影响，在缓存命中率高的场景中，这种策略会错失大量加速机会。

\textbf{ai-Dynamo与Company-X}采用线性组合策略。调度器维护两个指标：缓存命中率（或命中节省的Prefill token数）和实例负载（batch size或总token数）。得分计算为二者的加权和，通过调整权重控制两个目标的侧重。这类策略能够兼顾缓存感知和负载均衡，但权重需要针对特定workload调优。

\textbf{AIBrix}采用过滤式组合。调度器先检查集群负载是否失衡，若负载差异超过阈值则优先负载均衡，否则基于缓存命中率选实例。阈值同样需要人工设定。

\textbf{动机实验：Company-X调参敏感性分析}为说明线性组合方法的局限，我们在Qwen-Chat workload上改变Company-X的权重参数进行实验。实验固定硬件为8×H20，模型为Qwen3-30B-A3B，请求trace为20分钟真实对话请求。权重参数λ从0.3变化至0.9（λ越靠近0越偏向负载均衡，越靠近1越偏向缓存感知）。实验发现：最优λ值随workload变化，对话场景最优λ为0.7，代码场景最优λ为0.5，智能体场景最优λ为0.6。同一场景下，λ偏离最优值0.1导致平均TTFT增加15\%-25\%。这表明静态调参难以适应workload变化，而动态调整又缺乏明确指导原则。

\subsection{对未来状态或性能Metrics的预测}

第二类方法尝试预测请求指派后的执行性能，以预测值作为调度依据。这类方法不直接组合指标，而是通过模型或模拟器预估请求在各实例上的TTFT，选择预估最优的实例。

\textbf{llm-d}是代表性工作。调度器维护每个实例的KVCache状态和队列信息，对新到达请求，预估其在各实例上的TTFT（考虑缓存命中可能带来的加速），选择预估TTFT最小的实例。预估基于简化的性能模型，考虑输入长度、缓存命中长度、队列中等待请求等因素。

\textbf{Mooncake}以KVCache为中心设计调度。调度器将请求按前缀分类，优先将请求路由至缓存该前缀的实例；若这些实例负载过重，则考虑将请求发往无缓存的实例并接受重计算开销。Mooncake同样需要预估不同选择的代价，但其核心是缓存管理而非通用性能预测。

预测类方法的优势在于：预估的TTFT天然融合了负载均衡和缓存感知两个目标。负载重的实例即使缓存命中率高，其预估TTFT也可能因排队而变高；缓存命中率低的实例即使负载轻，预估Prefill时间也可能较长。调度器只需比较单一数值即可完成决策，无需人工设定权重或阈值。

但预测类方法面临实现层面的挑战。如何在在线场景中快速获得准确预估？如何保证预估使用的实例状态与真实状态保持一致？预测不准确会带来多大影响？这些问题在llm-d和Mooncake的论文中未被深入讨论。

\subsection{两类方法的根本差异}

两类方法的差异本质在于决策依据的时间维度。当前Metrics方法依赖请求到达时刻的瞬时状态，这些状态是历史积累的结果，但未必能准确反映请求未来的执行体验。预测类方法尝试预估请求的未来执行情况，直接面向决策目标，但需要付出额外的计算和同步成本。

从信息利用角度看，当前Metrics方法丢弃了大量信息：batch size相同但Prefill/Decode配比不同的两个实例被同等对待，尽管它们的服务能力可能相差甚远。预测类方法通过模拟或建模，尽可能保留和执行相关的信息，但信息保留越完整，计算开销越高。

\section{现有方法未解决的问题}

现有工作虽已尝试将性能预测引入调度决策，但三个关键问题仍未得到清晰解答。

第一，在线场景中如何实现可用的性能预测？预测依赖实例状态，但分布式系统固有的观测时延和处理顺序不确定性导致状态同步困难。调度器维护的模拟状态如何与真实实例状态保持一致？预测器如何设计才能在在线路径中稳定运行？这些问题在llm-d等工作中未被阐明。

第二，预测能力能带来多大的调度性能提升？已有工作未进行端到端对比实验，验证基于预测的调度与经典调度策略的实际差距。在真实硬件、真实workload上，预测类方法相比vLLM、ai-Dynamo等能否带来可量化的TTFT/TPOT改善？

第三，预测质量如何影响调度效果？预测不可能完全准确，但预测误差在多大范围内可接受？预测吞吐是否成为系统瓶颈？预测准确性和吞吐之间存在权衡，如何取舍才能最大化端到端服务质量？

本文面向PD共置部署形态，尝试填补上述空白。我们设计并实现在线模拟器，解决状态同步与开销控制问题；通过端到端实验量化预测类方法的性能提升；系统分析预测准确性和吞吐对调度效果的影响，为推理集群调度设计提供实证依据。