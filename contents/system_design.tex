% !TEX root = ../main.tex

\chapter{系统设计}
\label{ch:design}

离线模拟器是LLM服务系统设计与容量规划的重要工具。给定完整的请求trace和系统配置，
离线模拟器在CPU上步进执行，输出吞吐、时延等性能指标，以评估不同配置的效果。
这类模拟器的运行环境是单机的，状态确定性的线性发展，没有实时性要求。
而在线调度场景的性能预测能力对模拟器提出了完全不同的需求，
模拟器需要在每个请求到达时，快速得到请求在当前实例的执行性能。
这要求模拟器以在线服务的方式运行，与推理实例分布式部署和状态同步，
对请求预测能够实时响应，并达到微秒级预测延迟。

将离线模拟器直接用于在线调度面临三个根本性差异。

第一，接口和语义不同。离线模拟器通常提供批处理接口：
输入trace文件，输出性能报告。而在线调度需要的是低延迟的查询接口，
能够为每个到达的请求并行查询多个实例，并在微秒级返回预测结果。

第二，状态维护不同。离线模拟器中，模拟器完全决定实例状态，
所有信息都在单进程内同步更新。在线场景中，调度器和实例是分布式部署的，
并且实例调度器为了优化吞吐、公平性等目标，
正在采用越来越复杂的设计——优先级抢占、动态批处理、分块预填充、请求优先级队列。
这些行为为调度引入了不确定性和复杂性，也为模拟器状态同步带来了挑战。

第三，执行过程不同。在线调度需要回答的是“如果把这个请求发过去会怎样”。
这是反事实的执行过程，需要在当前已经发生的状态之上，叠加一个尚未发生的请求，
向前模拟直到该请求产出首token。
这引出了性能预测模块模拟器的设计目标: 
首先，模拟器的状态能够与推理实例的状态保持同步，以保证对请求的模拟预测能够准确。
其次，由于请求不确定被调度到模拟的实例，因此预测过程中的状态演进不能影响真实状态，导致后续请求模拟受到干扰。
最后，在预测时延上，由于性能预测过程处于在线服务场景全局调度的关键路径上，需要高效地完成预测，以保证高请求到达率下预测模块不会成为推理集群的性能瓶颈。
在本章节中将详细分析在线性能预测模块的系统设计，重点介绍状态同步机制和状态演进管理的设计与实现。

\section{架构与接口设计}

推理服务集群和性能预测模块的整体架构如图(TODO!)所示。在整个推理服务集群中，全局调度器作为集群入口，接收所有外部的请求，其中的性能预测模块维护一组模拟器实例，
每个模拟器对应一个推理实例，通过对请求到达对应实例后的推理性能进行预测，返回性能指标指导调度决策。
推理实例与全局调度器之间维持两条通信通道: 调度器完成调度决策后将请求指派到推理实例执行，
推理实例在每次批处理结束时将状态更新和调度行为通过SSE长连接上报到全局调度器，以将该轮批处理生成的Token推送给用户、
修改全局调度器中维护的实例指标和触发性能预测模块中对应模拟器状态的同步。

调度器通过三个接口与性能预测组件交互，包括\texttt{Predict}、\texttt{Add\_request}和\texttt{Sync\_State}。
其中\texttt{Add\_request}确认请求指派、\texttt{Sync\_State}由推理实例上报，用于同步实例批处理的行为，这两个接口修改模拟器状态，实现与推理实例的状态同步
而\texttt{Predict}作为核心接口，对新到达的请求进行性能预测，并向全局调度器返回性能指标。
在本文中，以TTFT作为模拟器返回的性能指标，即预测新到达请求在当前实例执行完Prefill阶段的时长
由于调度器需要为多个实例进行预测以进行比较，\texttt{Predict}支持并发调用对多个实例进行预测。

\section{状态同步机制}

忽略由生成Token的不同带来的随机性，常见的实例调度器批处理行为通常是确定的，即在相同的状态下能够构造出完全相同的批，从而得到相近的执行时间，这是模拟器能够对LLM实例或集群推理性能进行相对准确预测的前提。

在离线模拟器中，通过在给定Workload中包含请求生成长度、生成Token信息和请求到达时间信息来排除请求到达和生成Token的随机性。
预测过程中，由模拟器完全控制实例状态。具体来说，执行队列、等待队列和KVCache等状态由模拟器管理，
并随事件循环的时间推进而模拟调度执行和状态更新。这意味这状态管理是确定性线性推进的。
而在在线场景中，全局调度器和推理实例是分离的组件，由全局调度器维护每个实例的状态镜像，
为了保证性能预测的准确，需要保证两者基于相同的状态。

\subsection{状态同步的必要性}
推理实例根据动态生成的Token控制请求终止(例如EOF或用户指定的终止字符)，
推理实例比全局调度器包含更多确定性信息，模拟器需要同步推理实例的最新状态来消除不确定性。

并且实例调度器正在变得越来越智能，一些研究工作引入了优先级抢占、请求暂停与恢复、公平性队列等更复杂的行为。
这些行为在实例内部是确定性的，但从外部观察，全局调度器很难仅凭自管的信息来还原实例的完整状态和行为。

由于上述在线场景下全局调度器的信息缺失和职责不同，依赖全局调度器去猜测实例的状态变化是不可行的。
需要让实例主动将状态变更和执行行为上报给调度器以实现状态同步机制。

\subsection{同步接口设计}

常见的推理框架通过OpenAI接口接收推理请求，但全局调度器不便于通过OpenAI接口返回的请求信息得知推理实例的执行行为。
因此为推理框架添加新的SSE长连接接口，启动时全局调度器与每个实例建立连接。
在推理实例每次发生影响未来调度的状态变化时，将调度结果、执行结果的关键信息上报给全局调度器。

推理实例状态变化在两个时间点发生：第一是接收到全局调度器指派的新请求并加入请求队列，此时只需要在模拟器中也将新请求添加到请求队列。
第二是推理实例每次批处理结束时,此时需要更为精细的状态上报与更新。
状态上报的设计需要权衡信息量和通信开销。对于完整的状态快照，因为包含所有KVCache的分配信息，信息量由于过于昂贵不可接受。
本文采用增量更新上报的方式：上报的信息只包含自上次上报以来发生变化的部分。
例如，当前批处理包含的请求ID和关键信息（处理长度、新生成的Token以及是否完成）、
KVCache的变化(以vLLM为例，包括新分配的Block ID、对应的Hash和剔除的Block ID)以及特殊事件的记录例如请求抢占等。

而对于全局调度器，则需要根据上报信息更新每个实例的状态镜像，维护请求队列和KVCache状态的镜像。

\subsection{同步机制的代价}
对于推理框架添加同步机制，带来的额外处理和网络开销非常小。
这是因为摘要体积很小，通常只有几十K字节，在推理实例调度和批处理结束状态更新时插桩，对网络和实例处理的开销可以忽略(TODO! 可以测一下实例处理的开销)。
并且状态上报的频率和推理实例的批处理频率相当，以实验的H20机器为例，根据批大小和请求上下文长度不同，上报间隔通常在几十毫秒到几百毫秒之间。
在全局调度器上的平均处理时延为(TODO!),完全能够满足在线调度的需求，不会阻塞调度决策。

状态同步对实例的改动也很小，主要是状态变更点的埋点和添加状态上报的接口，在vLLM中实现状态同步增加约x行代码。

\section{状态演进管理}
与离线的模拟器不同，在线模拟器的状态存在反事实的预测过程。
每次收到新请求时，模拟器会模拟这个新请求在实例状态上的调度，这会导致实例状态如请求队列、KVCache的改变。
在这个过程并不应影响模拟器维护的真实状态，因为如果该请求没有被指派到该实例，后续请求的预测从正确的状态开始。
因此需要对预测过程中的状态演进进行管理，保证不会影响到真实状态。

\subsection{直接的做法: 独立预测}

最简单直接的做法是：每次调用Predict接口，模拟器都从当前状态镜像S开始，克隆一份请求队列作为预测起点。在这个克隆队列上插入新请求开始模拟调度，直到新请求完成Prefill。模拟结束后将克隆队列丢弃，从而保持模拟器的真实状态不受影响。
如(TODO!)所示，考虑连续到达的两个请求r1、r2。
假设r1到达时状态为S0，我们模拟得到它的TTFT，同时模拟过程会从S0推进到S1（经过若干步）。
如果r2到达时实例状态仍然是S0（r1还在排队，尚未开始执行），按照独立预测的做法，我们克隆出的状态从S0开始模拟r2的执行。
但注意到从S0推进到S1的过程实际上被重复计算了，r1之后若干步的模拟结果本可以被r2复用。

\subsection{优化: 预测复用}
独立预测对前序请求的重复预测带来的不必要的额外计算，这引出一个关键洞察：当前常见的推理框架的请求调度是遵循先到先服务的，新到达请求需要排在当前请求队列的末尾，
而新请求的执行状态演进路径必然包含前序未完成请求的演进路径，这为预测复用提供了可能。

因此可以定义三个状态: 
真实状态\(S\)，演进状态\(S'\)和模拟状态\(S''\)。真实状态定义为模拟器维护的、与推理实例当前状态对齐的状态镜像，通过Sync\_State接口更新。
将演进状态定义为模拟器模拟的完成所有前序请求Prefill时的推理实例状态，演进状态除了最终的请求队列状态外，还记录每次批处理构建的批和对应的预测执行时长，用于预测复用和后续的演进状态校准。
模拟状态则是在演进状态基础上，对新到达请求预测所使用的状态，在Predict调用时创建，调用结束后可根据请求是否被指派到该实例来决定丢弃该状态或将其作为新的演进状态。
这样通过维护一个最新的演进状态，来复用对前序请求Prefill的状态预测结果。

预测过程如(TODO!)，在初始调用\texttt{predict(r)}时，演进状态\(S'\)等同于真实状态\(S\)，从演进状态克隆出模拟状态\(S''\)，在\(S''\)上插入r并向前模拟，直到r的prefill完成并返回TTFT。
此时保留\(S''\)等待调度结果，如果调度器确认请求发往此实例，在调用Add\_request时将\(S''\)作为新的演进状态\(S'\)，作为后续预测的起点；如果请求没有被指派到此实例，则丢弃模拟状态\(S''\)，保持演进状态\(S'\)不变。
当请求r2到达时，演进状态\(S'\)已经包含了r1的prefill演进结果，因此在预测r2时可以直接从\(S'\)克隆出新的模拟状态\(S''\)，避免了对r1的重复预测。

\subsection{演进状态校准}
现实中，实例真实执行的状态推进可能因为一些原因与演进状态不一致。
考虑如下场景，实例内部对到达请求进行并行分词，长文本请求A先于短文本请求B到达推理实例，但请求B可能先于请求A完成分词进入实例调度器的等待队列，
造成队列顺序与到达顺序不一致。如果全局调度器在此时仍然使用先前的演进状态，就会与实例状态发生分歧，导致后续预测偏差。

因此需要通过状态同步机制校准演进状态，当调度器每次收到推理实例批处理完成后的状态同步时，在\texttt{Sync\_State}中与演进状态记录的第一个批进行比较，验证当前一轮批处理的预测与真实批处理结果是否匹配：
如果推理实例上报的批与演进状态中预测的批一致，说明这一步批处理的预测与真实执行路径没有发生偏离，将演进状态中该批移除，能够继续复用后续预测的批处理结果。
而如果当前一轮批处理的结果不一致(例如Decode请求数不同)，意味着演进状态的预测与真实执行路径发生了偏离。此时选择丢弃演进状态中保存的所有批处理预测结果，将演进状态重置为当前实例的真实状态。