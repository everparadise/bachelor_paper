% !TEX root = ../main.tex

\chapter{绪论}

\section{背景}
随着大语言模型（Large Language Models，LLMs）的涌现能力在智能对话、代码生成以及智能体系统等应用场景中的广泛验证，
各领域对大语言模型的调用需求呈几何倍数增长。然而用于大模型部署的硬件资源价格非常昂贵(TODO!),
绝大多数使用大模型的用户、企业难以独立部署所需的LLM推理服务，这为提供模型即服务的大模型厂商带来了飞速增长的模型调用需求，
因而模型服务厂商如何面向用户提供高效的推理服务成为当前AI系统架构的重要问题。

\subsection{LLM服务厂商部署方案}
在这些模型即服务厂商的实际部署中，为了满足用户海量请求调用对硬件和算力的需求，LLM 推理服务通常以集群形式运行，通过复制的方式高效、方便的扩展硬件来提高服务能力(如图)，
每个推理实例作为独立的模型部署单元，通过批处理的方式管理算力资源。
在推理实例之上，全局调度器（global scheduler）将到达集群的调用请求路由至合适的推理实例执行。
这对交互型的应用如ChatGPT、copilots提高用户体验非常重要，
也往往作为大模型服务厂商衡量集群服务能力和向用户承诺请求时延的重要指标。

\subsection{LLM性能模拟器}
LLM性能模拟器也是当前很多工作的关注点，用于分析和评估AI服务系统的性能。这类工作通常通过在 CPU 上模拟大规模 GPU 集群的执行过程，
在给定系统配置和请求 trace 的情况下，离线得到性能指标。根据面对的场景和特点可以进行如下分类
1. 训练性能模拟。 这类模拟器主要面向训练场景，较有影响力的工作例如SimAI，特点是使用重量级的网络模拟和较为简单的计算时长模拟。
因为训练场景由于Data parallelism的存在会产生大量的通信需求和网络开销；而对于计算，因为训练时的批处理模式较为固定，
同一批中的所有序列长度保持相同，使得不需要特别复杂的计算建模方式。
2. 离线推理模拟。这类模拟器面向推理场景，目标是模拟集群吞吐、优化集群配置。由于推理时不存在数据并行，网络建模方式较为简单，
而批处理和分块预填充这样的行为导致预测批处理的计算时延存在动态性，计算建模的难度更高。

通过对已有模拟器工作的分析，可以将模拟器拆分为两个组件，首先是用于状态管理、模拟调度行为的调度器(Scheduler)。这个组件维护了推理实例的状态信息，例如KVCache、请求队列等。
通过模拟推理实例的请求调度行为，按照步进的方式构造Batch。
由于常见的实例调度器的行为是确定性的(例如vLLM)，这是模拟器工作能够模拟实例调度行为的前提，保证了能够得到与真实实例调度相同的Batch构造结果。
不过在在线服务中，由于分布式系统中通信时延的不确定、组件间队列不保序和状态同步的滞后性，可能会导致模拟器状态与真实实例状态发生Diverge，发生的原因和处理方式会在后文(TODO!)中详细分析。

第二个组件是对给定的Batch预测得到执行时间的预测器(Predictor)。这个组件通过对当前批处理的特征进行建模，来预测得到对应的执行时间。
已有工作通过不同的性能模型(Performance Model)来建模批处理的执行时间，例如线性分析模型、神经网络、随机森林等。
要得到一个准确、高效的性能模型并不简单，需要对于预测特征进行仔细的选取和设计，以包含这个批的全部关键信息。
但是如何实现这样的性能模型不在本文的讨论范围内，本文的预测器实现参考当前使用较为广泛的推理模拟器系统Vidur来验证在线模拟器的设计和优化方法的有效性。

\subsection{调度策略}
在集群部署的全局调度器中，合理的调度策略能够通过负载均衡，充分利用不同实例的计算资源，
还能通过前缀匹配(TODO!)重用键值缓存避免重复计算，能够降低请求的推理时延，提高系统吞吐。
但是实现一个高效、兼顾负载均衡与前缀匹配的调度策略，实现系统服务能力的最优并不容易，已有的调度实现有不同的方式来解决这个问题，可以按照如下的方式进行分类。

首先是基于历史和当前的服务指标(例如Batch size, Total tokens)混合来描述实例负载和服务情况并进行筛选，这是最直接、简单的调度策略实现，
能够通过调整混合参数、筛选阈值来控制对负载均衡、前缀匹配的选择权重。在工业界、学术界服务系统中都有部署(TODO!)。
但是这样的指标难以捕捉推理实例中真实、全面的负载情况，并且在多个指标混合时的系数也难以确定，
往往通过网格搜索或人类专家启发式调优的方式设置。这样的调度策略实现难以调参，由于此带来的次优参数配置可能没有完全发掘推理系统的服务能力。

其次，有些系统尝试通过对性能指标进行预测(例如TTFT、TPOT)来指导请求调度决策(TODO!)，这种策略通过请求在对应实例的预测推理时延情况来进行调度决策。
这样的策略目标对请求执行性能进行选择，请求执行性能同时也能包含一定实例负载、前缀匹配的因素，从而间接达到兼顾集群负载均衡的目标。

但已有的基于预测性能的服务系统并未说明如何实现预测的能力，通过模拟器性能模拟的方式得到预估的请求推理性能是一个直接、自然的想法，但是基于模拟器实现性能预测调度策略的过程中存在很多问题未被阐明。
首先。在线服务场景有很多独特的特点，与离线的集群模拟存在很多不同，因此如何在在线服务场景中应用模拟器以及在性能预测过程中会遇到哪些困难，是首先会遇到的问题。
其次，性能预测能力是如何指导调度决策的，不同的预测准确性、吞吐会在多大程度上影响集群服务能力也仍未可知。
此外，基于这两种方式的调度策略并没有进行详实的比较分析，性能预测能力是否会带来集群服务能力的提升、是否值得投入人力实现复杂的模拟器进行预测，这也是推理集群设计面临的重要问题。




\section{主要贡献}
本文面向 LLM 推理集群的全局调度场景，主要解决的问题和贡献如下
1. 在在线服务场景中尝试应用推理模拟器来实现请求性能预测能力，并指导调度器的调度决策。通过合理的性能建模方式，在的准确性测试中达到了90\%的请求预测误差在15\%以内，
并针对预测误差进行详尽的分析和定位，发现了在线调度场景中由于分布式系统固有问题带来预测失真的独特原因。
2. 通过更换模拟器性能建模方式和吞吐优化方案，分析并阐明了在基于模拟器性能预测的调度策略中，模拟器预测的准确性和吞吐对端到端服务质量会带来怎样的影响。
以及模拟器为什么能够在在线调度中起到帮助。
3. 通过详尽、真实、可信的端到端测试，对比基于模拟器的策略与多项已有工作推理系统的调度策略(vLLM-v1、NVIDIA Dynamo 以及 llm-d)。
本文在不同硬件、模型和工作负载组合下进行对比实验，验证了基于模拟器的策略相比使用最广泛的开源服务系统vLLM最高能够带来(TODO)的Mean TTFT提升。
硬件环境包括8卡NVIDIA H20集群和8卡NVIDIA H100集群，模型配置包括 Qwen2.5-7B（Dense 模型）和 Qwen3-30B-A3B（MoE 模型，并在条件允许的情况下探索性扩展至 Qwen3-235B 等更大规模模型。 使用的工作负载包括 Qwen Chatbot、Qwen Coder、Qwen Agent和Kimi ToolAgent。

